{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from os.path import isfile\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Conv2D, BatchNormalization, Lambda\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras import backend\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from   torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from   torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Electronic', 1: 'Experimental', 2: 'Folk', 3: 'Hip-Hop', 4: 'Instrumental', 5: 'International', 6: 'Pop', 7: 'Rock'}\n"
     ]
    }
   ],
   "source": [
    "dict_genres = {'Electronic':0, 'Experimental':1, 'Folk':2, 'Hip-Hop':3, \n",
    "               'Instrumental':4,'International':5, 'Pop' :6, 'Rock': 7  }\n",
    "\n",
    "\n",
    "reverse_map = {v: k for k, v in dict_genres.items()}\n",
    "print(reverse_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arr_0', 'arr_1']\n",
      "(6394, 640, 128) (6394,)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load('shuffled_train.npz')\n",
    "print(npzfile.files)\n",
    "X_train = npzfile['arr_0']\n",
    "y_train = npzfile['arr_1']\n",
    "print(X_train.shape, y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arr_0', 'arr_1']\n",
      "(800, 640, 128) (800,)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load('shuffled_valid.npz')\n",
    "print(npzfile.files)\n",
    "X_valid = npzfile['arr_0']\n",
    "y_valid = npzfile['arr_1']\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arr_0', 'arr_1']\n",
      "(800, 640, 128) (800,)\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load('shuffled_test.npz')\n",
    "print(npzfile.files)\n",
    "X_test = npzfile['arr_0']\n",
    "y_test = npzfile['arr_1']\n",
    "print(X_valid.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.from_numpy(X_train).type(torch.Tensor)\n",
    "dev_X = torch.from_numpy(X_valid).type(torch.Tensor)\n",
    "test_X = torch.from_numpy(X_test).type(torch.Tensor)\n",
    "\n",
    "# Targets is a long tensor of size (N,) which tells the true class of the sample.\n",
    "train_Y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "dev_Y = torch.from_numpy(y_valid).type(torch.LongTensor)\n",
    "test_Y = torch.from_numpy(y_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'usqueeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-62ea532fabf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0ma_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompose1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_trans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mxtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_trans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mxtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'usqueeze'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "xtrain = torch.Tensor()\n",
    "compose = transforms.Compose([transforms.ToPILImage(),transforms.Resize((128,128),interpolation=Image.NEAREST)])\n",
    "compose1 = transforms.Compose([transforms.ToTensor()])\n",
    "train_X1 = train_X.unsqueeze(1).cpu()\n",
    "for imgdata in train_X1:\n",
    "    a_trans = compose(imgdata)\n",
    "    a_trans = compose1(a_trans)\n",
    "    xtrain = torch.cat((xtrain, a_trans),0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6394, 1, 128, 128])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = xtrain.unsqueeze(1)\n",
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = torch.Tensor()\n",
    "val_X1 = dev_X.unsqueeze(1).cpu()\n",
    "for imgdata in val_X1:\n",
    "    a_trans = compose(imgdata)\n",
    "    a_trans = compose1(a_trans)\n",
    "    xval = torch.cat((xval, a_trans),0)\n",
    "xval = xval.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = torch.Tensor()\n",
    "test_X1 = test_X.unsqueeze(1).cpu()\n",
    "for imgdata in test_X1:\n",
    "    a_trans = compose(imgdata)\n",
    "    a_trans = compose1(a_trans)\n",
    "    xtest = torch.cat((xtest, a_trans),0)\n",
    "xtest = xtest.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32*2*2\n",
    "num_classes = 8\n",
    "n_features = X_train.shape[2]\n",
    "n_time = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnnrnn(nn.Module):\n",
    "    def __init__(self, embed_size = 128, num_layers = 4, output_dim = 8, batchsize = 100):\n",
    "        super(cnnrnn,self).__init__()\n",
    "       \n",
    "        '''Encoder'''\n",
    "       \n",
    "        # conv1d layers\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(1, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            #nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(64 * 8, 64*16, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            #nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*16,128),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # add another fully connected layer\n",
    "        self.embed = nn.Linear(in_features=128, out_features=128)\n",
    "       \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "       \n",
    "        # activation layers\n",
    "        self.prelu = nn.PReLU()\n",
    "       \n",
    "        '''Decoder'''\n",
    "       \n",
    "        self.hidden_dim1 = 128\n",
    "        self.hidden_dim2 = 64\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.batch_size = batchsize\n",
    "       \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(embed_size, self.hidden_dim1, self.num_layers)\n",
    "\n",
    "        # output layer\n",
    "        self.linear1 = nn.Linear(self.hidden_dim1, self.hidden_dim2)\n",
    "       \n",
    "        # output layer\n",
    "        self.linear2 = nn.Linear(self.hidden_dim2, self.output_dim)\n",
    "       \n",
    "        # dropout layer\n",
    "        self.dropout1 = nn.Dropout(p=0.4)\n",
    "   \n",
    "    def init_hidden(self):\n",
    "        return (\n",
    "            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim1),\n",
    "            torch.zeros(self.num_layers, self.batch_size, self.hidden_dim1),\n",
    "        )\n",
    "       \n",
    "    def cnnencoder(self, images):\n",
    "        x = self.conv1(images)\n",
    "        \n",
    "        # get the embeddings from the densenet\n",
    "        convnet_outputs = self.dropout(self.prelu(images))\n",
    "       \n",
    "        # pass through the fully connected\n",
    "        embeddings = self.embed(convnet_outputs)\n",
    "       \n",
    "        return embeddings\n",
    "   \n",
    "    def rnndecoder(self, features):\n",
    "        lstm_out, hidden = self.lstm(features.squeeze())\n",
    "        logits1 = self.dropout1(self.linear1(lstm_out[-1]))\n",
    "        logits = self.linear2(logits1)\n",
    "        genre_scores = F.log_softmax(logits, dim=1)\n",
    "        return genre_scores\n",
    "   \n",
    "    def forward(self, inputs):\n",
    "        embed = self.cnnencoder(inputs)\n",
    "        output = self.rnndecoder(embed)\n",
    "        return output\n",
    "   \n",
    "    def get_accuracy(self, logits, target):\n",
    "        \"\"\" compute accuracy for training round \"\"\"\n",
    "        corrects = (\n",
    "            torch.max(logits, 1)[1].view(target.size()).data == target.data\n",
    "        ).sum()\n",
    "        accuracy = 100.0 * corrects / self.batch_size\n",
    "        return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on GPU\n"
     ]
    }
   ],
   "source": [
    "model = cnnrnn(embed_size = 128, num_layers = 2, output_dim = 8, batchsize = batch_size).cuda()\n",
    "loss_function = nn.NLLLoss()  # expects ouputs from LogSoftmax\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"\\nTraining on GPU\")\n",
    "else:\n",
    "    print(\"\\nNo GPU, training on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all training data (epoch) / batch_size == num_batches (12)\n",
    "num_batches = int(train_X.shape[0] / batch_size)\n",
    "num_dev_batches = int(dev_X.shape[0] / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_list, val_accuracy_list, epoch_list = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xtrain.cuda()\n",
    "train_Y = train_Y.cuda()\n",
    "xval = xval.cuda()\n",
    "dev_Y = dev_Y.cuda()\n",
    "xtest = xtest.cuda()\n",
    "test_Y = test_Y.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | NLLoss: 2.0827 | Train Accuracy: 11.78\n",
      "Validation ...\n",
      "Epoch:  0 | NLLoss: 2.0827 | Train Accuracy: 11.78 | Val Loss 2.0865  | Val Accuracy: 6.00\n",
      "Epoch:  1 | NLLoss: 2.0814 | Train Accuracy: 11.67\n",
      "Validation ...\n",
      "Epoch:  2 | NLLoss: 2.0802 | Train Accuracy: 12.08\n",
      "Validation ...\n",
      "Epoch:  3 | NLLoss: 2.0805 | Train Accuracy: 12.12\n",
      "Validation ...\n",
      "Epoch:  4 | NLLoss: 2.0805 | Train Accuracy: 11.41\n",
      "Validation ...\n",
      "Epoch:  5 | NLLoss: 2.0806 | Train Accuracy: 11.43\n",
      "Validation ...\n",
      "Epoch:  6 | NLLoss: 2.0797 | Train Accuracy: 12.10\n",
      "Validation ...\n",
      "Epoch:  7 | NLLoss: 2.0800 | Train Accuracy: 11.78\n",
      "Validation ...\n",
      "Epoch:  8 | NLLoss: 2.0799 | Train Accuracy: 12.35\n",
      "Validation ...\n",
      "Epoch:  9 | NLLoss: 2.0796 | Train Accuracy: 12.53\n",
      "Validation ...\n",
      "Epoch:  10 | NLLoss: 2.0801 | Train Accuracy: 12.06\n",
      "Validation ...\n",
      "Epoch:  10 | NLLoss: 2.0801 | Train Accuracy: 12.06 | Val Loss 2.0809  | Val Accuracy: 14.00\n",
      "Epoch:  11 | NLLoss: 2.0802 | Train Accuracy: 11.90\n",
      "Validation ...\n",
      "Epoch:  12 | NLLoss: 2.0801 | Train Accuracy: 12.51\n",
      "Validation ...\n",
      "Epoch:  13 | NLLoss: 2.0791 | Train Accuracy: 13.08\n",
      "Validation ...\n",
      "Epoch:  14 | NLLoss: 2.0793 | Train Accuracy: 12.61\n",
      "Validation ...\n",
      "Epoch:  15 | NLLoss: 2.0782 | Train Accuracy: 12.98\n",
      "Validation ...\n",
      "Epoch:  16 | NLLoss: 2.0786 | Train Accuracy: 13.92\n",
      "Validation ...\n",
      "Epoch:  17 | NLLoss: 2.0799 | Train Accuracy: 12.20\n",
      "Validation ...\n",
      "Epoch:  18 | NLLoss: 2.0791 | Train Accuracy: 12.71\n",
      "Validation ...\n",
      "Epoch:  19 | NLLoss: 2.0774 | Train Accuracy: 12.80\n",
      "Validation ...\n",
      "Epoch:  20 | NLLoss: 2.0787 | Train Accuracy: 13.16\n",
      "Validation ...\n",
      "Epoch:  20 | NLLoss: 2.0787 | Train Accuracy: 13.16 | Val Loss 2.0827  | Val Accuracy: 7.00\n",
      "Epoch:  21 | NLLoss: 2.0779 | Train Accuracy: 13.39\n",
      "Validation ...\n",
      "Epoch:  22 | NLLoss: 2.0776 | Train Accuracy: 13.27\n",
      "Validation ...\n",
      "Epoch:  23 | NLLoss: 2.0774 | Train Accuracy: 13.33\n",
      "Validation ...\n",
      "Epoch:  24 | NLLoss: 2.0768 | Train Accuracy: 13.08\n",
      "Validation ...\n",
      "Epoch:  25 | NLLoss: 2.0763 | Train Accuracy: 12.84\n",
      "Validation ...\n",
      "Epoch:  26 | NLLoss: 2.0774 | Train Accuracy: 13.71\n",
      "Validation ...\n",
      "Epoch:  27 | NLLoss: 2.0763 | Train Accuracy: 13.35\n",
      "Validation ...\n",
      "Epoch:  28 | NLLoss: 2.0757 | Train Accuracy: 13.24\n",
      "Validation ...\n",
      "Epoch:  29 | NLLoss: 2.0745 | Train Accuracy: 14.12\n",
      "Validation ...\n",
      "Epoch:  30 | NLLoss: 2.0754 | Train Accuracy: 12.92\n",
      "Validation ...\n",
      "Epoch:  30 | NLLoss: 2.0754 | Train Accuracy: 12.92 | Val Loss 2.0707  | Val Accuracy: 9.00\n",
      "Epoch:  31 | NLLoss: 2.0750 | Train Accuracy: 14.22\n",
      "Validation ...\n",
      "Epoch:  32 | NLLoss: 2.0759 | Train Accuracy: 13.45\n",
      "Validation ...\n",
      "Epoch:  33 | NLLoss: 2.0744 | Train Accuracy: 13.61\n",
      "Validation ...\n",
      "Epoch:  34 | NLLoss: 2.0740 | Train Accuracy: 14.08\n",
      "Validation ...\n",
      "Epoch:  35 | NLLoss: 2.0734 | Train Accuracy: 13.49\n",
      "Validation ...\n",
      "Epoch:  36 | NLLoss: 2.0741 | Train Accuracy: 13.92\n",
      "Validation ...\n",
      "Epoch:  37 | NLLoss: 2.0708 | Train Accuracy: 14.24\n",
      "Validation ...\n",
      "Epoch:  38 | NLLoss: 2.0736 | Train Accuracy: 14.41\n",
      "Validation ...\n",
      "Epoch:  39 | NLLoss: 2.0722 | Train Accuracy: 14.55\n",
      "Validation ...\n",
      "Epoch:  40 | NLLoss: 2.0704 | Train Accuracy: 14.67\n",
      "Validation ...\n",
      "Epoch:  40 | NLLoss: 2.0704 | Train Accuracy: 14.67 | Val Loss 2.0625  | Val Accuracy: 14.00\n",
      "Epoch:  41 | NLLoss: 2.0702 | Train Accuracy: 14.31\n",
      "Validation ...\n",
      "Epoch:  42 | NLLoss: 2.0710 | Train Accuracy: 14.27\n",
      "Validation ...\n",
      "Epoch:  43 | NLLoss: 2.0676 | Train Accuracy: 15.47\n",
      "Validation ...\n",
      "Epoch:  44 | NLLoss: 2.0682 | Train Accuracy: 14.92\n",
      "Validation ...\n",
      "Epoch:  45 | NLLoss: 2.0693 | Train Accuracy: 14.45\n",
      "Validation ...\n",
      "Epoch:  46 | NLLoss: 2.0655 | Train Accuracy: 14.94\n",
      "Validation ...\n",
      "Epoch:  47 | NLLoss: 2.0701 | Train Accuracy: 14.71\n",
      "Validation ...\n",
      "Epoch:  48 | NLLoss: 2.0688 | Train Accuracy: 15.39\n",
      "Validation ...\n",
      "Epoch:  49 | NLLoss: 2.0678 | Train Accuracy: 15.47\n",
      "Validation ...\n",
      "Epoch:  50 | NLLoss: 2.0657 | Train Accuracy: 15.08\n",
      "Validation ...\n",
      "Epoch:  50 | NLLoss: 2.0657 | Train Accuracy: 15.08 | Val Loss 2.0494  | Val Accuracy: 15.00\n",
      "Epoch:  51 | NLLoss: 2.0653 | Train Accuracy: 15.33\n",
      "Validation ...\n",
      "Epoch:  52 | NLLoss: 2.0644 | Train Accuracy: 15.82\n",
      "Validation ...\n",
      "Epoch:  53 | NLLoss: 2.0692 | Train Accuracy: 14.69\n",
      "Validation ...\n",
      "Epoch:  54 | NLLoss: 2.0667 | Train Accuracy: 15.16\n",
      "Validation ...\n",
      "Epoch:  55 | NLLoss: 2.0650 | Train Accuracy: 15.98\n",
      "Validation ...\n",
      "Epoch:  56 | NLLoss: 2.0664 | Train Accuracy: 15.65\n",
      "Validation ...\n",
      "Epoch:  57 | NLLoss: 2.0635 | Train Accuracy: 17.00\n",
      "Validation ...\n",
      "Epoch:  58 | NLLoss: 2.0654 | Train Accuracy: 15.63\n",
      "Validation ...\n",
      "Epoch:  59 | NLLoss: 2.0645 | Train Accuracy: 15.04\n",
      "Validation ...\n",
      "Epoch:  60 | NLLoss: 2.0640 | Train Accuracy: 15.76\n",
      "Validation ...\n",
      "Epoch:  60 | NLLoss: 2.0640 | Train Accuracy: 15.76 | Val Loss 2.0423  | Val Accuracy: 18.00\n",
      "Epoch:  61 | NLLoss: 2.0651 | Train Accuracy: 15.65\n",
      "Validation ...\n",
      "Epoch:  62 | NLLoss: 2.0651 | Train Accuracy: 15.51\n",
      "Validation ...\n",
      "Epoch:  63 | NLLoss: 2.0640 | Train Accuracy: 16.06\n",
      "Validation ...\n",
      "Epoch:  64 | NLLoss: 2.0617 | Train Accuracy: 16.04\n",
      "Validation ...\n",
      "Epoch:  65 | NLLoss: 2.0599 | Train Accuracy: 16.41\n",
      "Validation ...\n",
      "Epoch:  66 | NLLoss: 2.0612 | Train Accuracy: 16.06\n",
      "Validation ...\n",
      "Epoch:  67 | NLLoss: 2.0632 | Train Accuracy: 15.92\n",
      "Validation ...\n",
      "Epoch:  68 | NLLoss: 2.0604 | Train Accuracy: 15.88\n",
      "Validation ...\n",
      "Epoch:  69 | NLLoss: 2.0593 | Train Accuracy: 16.88\n",
      "Validation ...\n",
      "Epoch:  70 | NLLoss: 2.0582 | Train Accuracy: 16.45\n",
      "Validation ...\n",
      "Epoch:  70 | NLLoss: 2.0582 | Train Accuracy: 16.45 | Val Loss 2.0281  | Val Accuracy: 20.00\n",
      "Epoch:  71 | NLLoss: 2.0619 | Train Accuracy: 15.90\n",
      "Validation ...\n",
      "Epoch:  72 | NLLoss: 2.0575 | Train Accuracy: 16.41\n",
      "Validation ...\n",
      "Epoch:  73 | NLLoss: 2.0559 | Train Accuracy: 16.98\n",
      "Validation ...\n",
      "Epoch:  74 | NLLoss: 2.0625 | Train Accuracy: 15.57\n",
      "Validation ...\n",
      "Epoch:  75 | NLLoss: 2.0654 | Train Accuracy: 16.12\n",
      "Validation ...\n",
      "Epoch:  76 | NLLoss: 2.0586 | Train Accuracy: 17.04\n",
      "Validation ...\n",
      "Epoch:  77 | NLLoss: 2.0582 | Train Accuracy: 16.14\n",
      "Validation ...\n",
      "Epoch:  78 | NLLoss: 2.0569 | Train Accuracy: 17.16\n",
      "Validation ...\n",
      "Epoch:  79 | NLLoss: 2.0525 | Train Accuracy: 17.55\n",
      "Validation ...\n",
      "Epoch:  80 | NLLoss: 2.0588 | Train Accuracy: 16.57\n",
      "Validation ...\n",
      "Epoch:  80 | NLLoss: 2.0588 | Train Accuracy: 16.57 | Val Loss 2.0118  | Val Accuracy: 25.00\n",
      "Epoch:  81 | NLLoss: 2.0534 | Train Accuracy: 16.41\n",
      "Validation ...\n",
      "Epoch:  82 | NLLoss: 2.0587 | Train Accuracy: 16.57\n",
      "Validation ...\n",
      "Epoch:  83 | NLLoss: 2.0568 | Train Accuracy: 16.73\n",
      "Validation ...\n",
      "Epoch:  84 | NLLoss: 2.0499 | Train Accuracy: 17.33\n",
      "Validation ...\n",
      "Epoch:  85 | NLLoss: 2.0507 | Train Accuracy: 16.88\n",
      "Validation ...\n",
      "Epoch:  86 | NLLoss: 2.0563 | Train Accuracy: 16.22\n",
      "Validation ...\n",
      "Epoch:  87 | NLLoss: 2.0598 | Train Accuracy: 17.04\n",
      "Validation ...\n",
      "Epoch:  88 | NLLoss: 2.0566 | Train Accuracy: 16.49\n",
      "Validation ...\n",
      "Epoch:  89 | NLLoss: 2.0546 | Train Accuracy: 16.73\n",
      "Validation ...\n",
      "Epoch:  90 | NLLoss: 2.0534 | Train Accuracy: 16.18\n",
      "Validation ...\n",
      "Epoch:  90 | NLLoss: 2.0534 | Train Accuracy: 16.18 | Val Loss 1.9888  | Val Accuracy: 25.00\n",
      "Epoch:  91 | NLLoss: 2.0536 | Train Accuracy: 16.69\n",
      "Validation ...\n",
      "Epoch:  92 | NLLoss: 2.0553 | Train Accuracy: 16.57\n",
      "Validation ...\n",
      "Epoch:  93 | NLLoss: 2.0521 | Train Accuracy: 16.86\n",
      "Validation ...\n",
      "Epoch:  94 | NLLoss: 2.0552 | Train Accuracy: 16.65\n",
      "Validation ...\n",
      "Epoch:  95 | NLLoss: 2.0527 | Train Accuracy: 17.12\n",
      "Validation ...\n",
      "Epoch:  96 | NLLoss: 2.0513 | Train Accuracy: 16.84\n",
      "Validation ...\n",
      "Epoch:  97 | NLLoss: 2.0528 | Train Accuracy: 16.76\n",
      "Validation ...\n",
      "Epoch:  98 | NLLoss: 2.0452 | Train Accuracy: 17.20\n",
      "Validation ...\n",
      "Epoch:  99 | NLLoss: 2.0515 | Train Accuracy: 18.20\n",
      "Validation ...\n",
      "Epoch:  100 | NLLoss: 2.0514 | Train Accuracy: 17.76\n",
      "Validation ...\n",
      "Epoch:  100 | NLLoss: 2.0514 | Train Accuracy: 17.76 | Val Loss 1.9783  | Val Accuracy: 29.00\n",
      "Epoch:  101 | NLLoss: 2.0453 | Train Accuracy: 18.31\n",
      "Validation ...\n",
      "Epoch:  102 | NLLoss: 2.0545 | Train Accuracy: 16.84\n",
      "Validation ...\n",
      "Epoch:  103 | NLLoss: 2.0554 | Train Accuracy: 17.06\n",
      "Validation ...\n",
      "Epoch:  104 | NLLoss: 2.0465 | Train Accuracy: 17.65\n",
      "Validation ...\n",
      "Epoch:  105 | NLLoss: 2.0537 | Train Accuracy: 16.65\n",
      "Validation ...\n",
      "Epoch:  106 | NLLoss: 2.0490 | Train Accuracy: 16.92\n",
      "Validation ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  107 | NLLoss: 2.0511 | Train Accuracy: 16.92\n",
      "Validation ...\n",
      "Epoch:  108 | NLLoss: 2.0515 | Train Accuracy: 17.29\n",
      "Validation ...\n",
      "Epoch:  109 | NLLoss: 2.0490 | Train Accuracy: 17.12\n",
      "Validation ...\n",
      "Epoch:  110 | NLLoss: 2.0452 | Train Accuracy: 17.35\n",
      "Validation ...\n",
      "Epoch:  110 | NLLoss: 2.0452 | Train Accuracy: 17.35 | Val Loss 1.9517  | Val Accuracy: 34.00\n",
      "Epoch:  111 | NLLoss: 2.0447 | Train Accuracy: 16.88\n",
      "Validation ...\n",
      "Epoch:  112 | NLLoss: 2.0467 | Train Accuracy: 17.45\n",
      "Validation ...\n",
      "Epoch:  113 | NLLoss: 2.0514 | Train Accuracy: 16.76\n",
      "Validation ...\n",
      "Epoch:  114 | NLLoss: 2.0511 | Train Accuracy: 16.73\n",
      "Validation ...\n",
      "Epoch:  115 | NLLoss: 2.0456 | Train Accuracy: 17.71\n",
      "Validation ...\n",
      "Epoch:  116 | NLLoss: 2.0502 | Train Accuracy: 17.71\n",
      "Validation ...\n",
      "Epoch:  117 | NLLoss: 2.0439 | Train Accuracy: 17.45\n",
      "Validation ...\n",
      "Epoch:  118 | NLLoss: 2.0465 | Train Accuracy: 17.65\n",
      "Validation ...\n",
      "Epoch:  119 | NLLoss: 2.0430 | Train Accuracy: 17.90\n",
      "Validation ...\n",
      "Epoch:  120 | NLLoss: 2.0465 | Train Accuracy: 17.02\n",
      "Validation ...\n",
      "Epoch:  120 | NLLoss: 2.0465 | Train Accuracy: 17.02 | Val Loss 1.9373  | Val Accuracy: 32.00\n",
      "Epoch:  121 | NLLoss: 2.0442 | Train Accuracy: 17.45\n",
      "Validation ...\n",
      "Epoch:  122 | NLLoss: 2.0455 | Train Accuracy: 17.10\n",
      "Validation ...\n",
      "Epoch:  123 | NLLoss: 2.0422 | Train Accuracy: 17.73\n",
      "Validation ...\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(2000):\n",
    "\n",
    "    train_running_loss, train_acc = 0.0, 0.0\n",
    "\n",
    "    # Init hidden state - if you don't want a stateful LSTM (between epochs)\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "\n",
    "        # zero out gradient, so they don't accumulate btw epochs\n",
    "        model.zero_grad()\n",
    "\n",
    "        # train_X shape: (total # of training examples, sequence_length, input_dim)\n",
    "        # train_Y shape: (total # of training examples, # output classes)\n",
    "        #\n",
    "        # Slice out local minibatches & labels => Note that we *permute* the local minibatch to\n",
    "        # match the PyTorch expected input tensor format of (sequence_length, batch size, input_dim)\n",
    "        X_local_minibatch, y_local_minibatch = (\n",
    "            xtrain[i * batch_size : (i + 1) * batch_size,],\n",
    "            train_Y[i * batch_size : (i + 1) * batch_size,],\n",
    "        )\n",
    "\n",
    "        # Reshape input & targets to \"match\" what the loss_function wants\n",
    "        #X_local_minibatch1 = X_local_minibatch\n",
    "\n",
    "        # NLLLoss does not expect a one-hot encoded vector as the target, but class indices\n",
    "        #y_local_minibatch = torch.max(y_local_minibatch)\n",
    "\n",
    "        y_pred = model(X_local_minibatch)                # fwd the bass (forward pass)\n",
    "        loss = loss_function(y_pred, y_local_minibatch)  # compute loss\n",
    "        loss.backward()                                  # reeeeewind (backward pass)\n",
    "        optimizer.step()                                 # parameter update\n",
    "\n",
    "        train_running_loss += loss.detach().item()       # unpacks the tensor into a scalar value\n",
    "        train_acc += model.get_accuracy(y_pred, y_local_minibatch)\n",
    "\n",
    "    print(\n",
    "        \"Epoch:  %d | NLLoss: %.4f | Train Accuracy: %.2f\"\n",
    "        % (epoch, train_running_loss / num_batches, train_acc / num_batches)\n",
    "    )\n",
    "\n",
    "    print(\"Validation ...\")  # should this be done every N epochs\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        val_running_loss, val_acc = 0.0, 0.0\n",
    "\n",
    "        # Compute validation loss, accuracy. Use torch.no_grad() & model.eval()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            model.hidden = model.init_hidden()\n",
    "            for i in range(num_dev_batches):\n",
    "                X_local_validation_minibatch, y_local_validation_minibatch = (\n",
    "                    xval[i * batch_size : (i + 1) * batch_size,],\n",
    "                    dev_Y[i * batch_size : (i + 1) * batch_size,],\n",
    "                )\n",
    "                #X_local_minibatch = X_local_validation_minibatch.permute(1, 0, 2)\n",
    "                #y_local_minibatch = torch.max(y_local_validation_minibatch)\n",
    "\n",
    "                y_pred = model(X_local_minibatch)\n",
    "                val_loss = loss_function(y_pred, y_local_minibatch)\n",
    "\n",
    "                val_running_loss += (\n",
    "                    val_loss.detach().item()\n",
    "                )  # unpacks the tensor into a scalar value\n",
    "                val_acc += model.get_accuracy(y_pred, y_local_minibatch)\n",
    "\n",
    "            model.train()  # reset to train mode after iterationg through validation data\n",
    "            print(\n",
    "                \"Epoch:  %d | NLLoss: %.4f | Train Accuracy: %.2f | Val Loss %.4f  | Val Accuracy: %.2f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    train_running_loss / num_batches,\n",
    "                    train_acc / num_batches,\n",
    "                    val_running_loss / num_dev_batches,\n",
    "                    val_acc / num_dev_batches,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        epoch_list.append(epoch)\n",
    "        val_accuracy_list.append(val_acc / num_dev_batches)\n",
    "        val_loss_list.append(val_running_loss / num_dev_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.train()\n",
    "    for i in range(num_batches):\n",
    "\n",
    "        # zero out gradient, so they don't accumulate btw epochs\n",
    "        model.zero_grad()\n",
    "\n",
    "        # train_X shape: (total # of training examples, sequence_length, input_dim)\n",
    "        # train_Y shape: (total # of training examples, # output classes)\n",
    "        #\n",
    "        # Slice out local minibatches & labels => Note that we *permute* the local minibatch to\n",
    "        # match the PyTorch expected input tensor format of (sequence_length, batch size, input_dim)\n",
    "        X_local_minibatch, y_local_minibatch = (\n",
    "            xtrain[i * batch_size : (i + 1) * batch_size,],\n",
    "            train_Y[i * batch_size : (i + 1) * batch_size,],\n",
    "        )\n",
    "\n",
    "        # Reshape input & targets to \"match\" what the loss_function wants\n",
    "        #X_local_minibatch1 = X_local_minibatch\n",
    "\n",
    "        # NLLLoss does not expect a one-hot encoded vector as the target, but class indices\n",
    "        #y_local_minibatch = torch.max(y_local_minibatch)\n",
    "\n",
    "        y_pred = model(X_local_minibatch)                # fwd the bass (forward pass)\n",
    "        loss = loss_function(y_pred, y_local_minibatch)  # compute loss\n",
    "        loss.backward()                                  # reeeeewind (backward pass)\n",
    "        optimizer.step()                                 # parameter update\n",
    "\n",
    "        train_running_loss += loss.detach().item()       # unpacks the tensor into a scalar value\n",
    "        train_acc += model.get_accuracy(y_pred, y_local_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203.0"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add another fully connected layer\n",
    "embed = nn.Linear(in_features=128, out_features=128).cuda()\n",
    "# dropout layer\n",
    "dropout = nn.Dropout(p=0.5).cuda()\n",
    "# activation layers\n",
    "prelu = nn.PReLU().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [128 x 5120], m2: [1024 x 128] at C:/w/1/s/tmp_conda_3.7_055457/conda/conda-bld/pytorch_1565416617654/work/aten/src\\THC/generic/THCTensorMathBlas.cu:273",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-35997edc3102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage_vectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1367\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1368\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1370\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [128 x 5120], m2: [1024 x 128] at C:/w/1/s/tmp_conda_3.7_055457/conda/conda-bld/pytorch_1565416617654/work/aten/src\\THC/generic/THCTensorMathBlas.cu:273"
     ]
    }
   ],
   "source": [
    "image_vectors = conv1(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embeddings from the densenet\n",
    "convnet_outputs = dropout(prelu(image_vectors))\n",
    "       \n",
    "# pass through the fully connected\n",
    "embeddings = embed(convnet_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = torch.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "compose = transforms.Compose([transforms.ToPILImage(),transforms.Resize((128,128),interpolation=Image.NEAREST)])\n",
    "compose1 = transforms.Compose([transforms.ToTensor()])\n",
    "#a_trans = compose(train_X.unsqueeze(1)[5].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-a6d285d94dba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0ma_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0ma_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompose1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_trans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mda\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma_trans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "da= torch.cat((a_trans.squeeze(1), a_trans.squeeze(1)),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([462, 128, 128])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "da.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fashiontest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize([0.5],[0.5]),\n",
    "                             ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-164-f31a464e564b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0msteps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;31m# Flatten MNIST images into a 784 long vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'steps' is not defined"
     ]
    }
   ],
   "source": [
    "for e in range(1):\n",
    "    running_loss = 0\n",
    "    for images, labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [200, 100]\n",
    "output_size = 10\n",
    "model = nn.Sequential(OrderedDict([\n",
    "                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "                      ('relu1', nn.ReLU()),\n",
    "                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "                      ('relu2', nn.ReLU()),\n",
    "                      ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "                      ('relu3', nn.ReLU())]))\n",
    "\n",
    "    \n",
    "# Traning Our Model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning Our Model\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "#print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
